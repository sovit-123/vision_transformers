{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5975d343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sovit/miniconda3/envs/vision_transformers/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from vision_transformers.models import mobile_vit\n",
    "from vision_transformers.utils import params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3b41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobile_vit.mobilevit_xs(num_classes=100, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f880999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileViT(\n",
      "  (conv_1): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (layer_1): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (exp_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (red_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_2): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (exp_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (red_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(128, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (exp_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (red_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (exp_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (red_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_3): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (exp_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "            (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (red_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): MobileViTBlock(\n",
      "      (local_rep): Sequential(\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (global_rep): Sequential(\n",
      "        (0): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=96, out_features=192, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=192, out_features=96, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=96, out_features=192, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=192, out_features=96, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (conv_proj): ConvBlock(\n",
      "        (block): Sequential(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion): ConvBlock(\n",
      "        (block): Sequential(\n",
      "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_4): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (exp_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (red_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): MobileViTBlock(\n",
      "      (local_rep): Sequential(\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(80, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (global_rep): Sequential(\n",
      "        (0): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=120, out_features=360, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=120, out_features=120, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=120, out_features=240, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=240, out_features=120, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=120, out_features=360, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=120, out_features=120, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=120, out_features=240, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=240, out_features=120, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=120, out_features=360, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=120, out_features=120, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=120, out_features=240, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=240, out_features=120, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=120, out_features=360, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=120, out_features=120, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=120, out_features=240, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=240, out_features=120, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (conv_proj): ConvBlock(\n",
      "        (block): Sequential(\n",
      "          (conv): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion): ConvBlock(\n",
      "        (block): Sequential(\n",
      "          (conv): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_5): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (exp_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=320, bias=False)\n",
      "            (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (red_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(320, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): MobileViTBlock(\n",
      "      (local_rep): Sequential(\n",
      "        (conv_3x3): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (conv_1x1): ConvBlock(\n",
      "          (block): Sequential(\n",
      "            (conv): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (global_rep): Sequential(\n",
      "        (0): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=144, out_features=144, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=144, out_features=288, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=288, out_features=144, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=144, out_features=144, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=144, out_features=288, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=288, out_features=144, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): TransformerEncoder(\n",
      "          (pre_norm_mha): Sequential(\n",
      "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Attention(\n",
      "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
      "              (out): Sequential(\n",
      "                (0): Linear(in_features=144, out_features=144, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (pre_norm_ffn): Sequential(\n",
      "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=144, out_features=288, bias=True)\n",
      "            (2): SiLU()\n",
      "            (3): Dropout(p=0.0, inplace=False)\n",
      "            (4): Linear(in_features=288, out_features=144, bias=True)\n",
      "            (5): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (conv_proj): ConvBlock(\n",
      "        (block): Sequential(\n",
      "          (conv): Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (fusion): ConvBlock(\n",
      "        (block): Sequential(\n",
      "          (conv): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_1x1_exp): ConvBlock(\n",
      "    (block): Sequential(\n",
      "      (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (global_pool): GlobalPool()\n",
      "    (dropout): Dropout(p=0.1, inplace=True)\n",
      "    (fc): Linear(in_features=384, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16885597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,971,348 total parameters.\n",
      "1,971,348 training parameters.\n"
     ]
    }
   ],
   "source": [
    "params.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4883be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
